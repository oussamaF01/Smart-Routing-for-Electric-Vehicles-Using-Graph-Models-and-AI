{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRQFkrLvxz-s",
        "outputId": "3fbc6fe8-3744-408f-a98c-7a63ef6d3f33"
      },
      "outputs": [],
      "source": [
        "!pip install osmnx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hff8-76SXQD-",
        "outputId": "7d03e59f-023c-4591-8933-ec1cb4c34c82"
      },
      "outputs": [],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF0cXZDIXZ2O",
        "outputId": "3599b837-bef1-4678-87c3-c42cdbd92c68"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX4AL98bYb9k",
        "outputId": "ef9a338a-a8c4-477b-fc43-c8bba34a3b4c"
      },
      "outputs": [],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fz1p4IEYXrxV"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import random\n",
        "from collections import deque\n",
        "import time\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "# Ensure TensorFlow doesn't consume all GPU memory\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QvJxkJTSXwQv"
      },
      "outputs": [],
      "source": [
        "def distance_between(graph, u, v):\n",
        "    \"\"\"Compute the length of the edge between u and v.\"\"\"\n",
        "    edge_data = graph.get_edge_data(u, v)\n",
        "    if edge_data and 'length' in edge_data:\n",
        "        return edge_data['length']\n",
        "    return float('inf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30fVdvZ_bB28"
      },
      "source": [
        "#GraphEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q8Skb35VXzvU"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GraphEnv(gym.Env):\n",
        "    def __init__(self, graph: nx.Graph, max_steps: int = 100,\n",
        "                 battery_start: float = 100.0, battery_max: float = 100.0,\n",
        "                 efficiency: float = 1.0, charging_nodes=[]):\n",
        "        super(GraphEnv, self).__init__()\n",
        "        self.start_node = None\n",
        "        self.goal_node = None\n",
        "        self.graph = graph\n",
        "        self.n_nodes = self.graph.number_of_nodes()\n",
        "        self.max_degree = max(dict(graph.degree()).values())\n",
        "        self.max_steps = max_steps\n",
        "        self.battery_start = battery_start\n",
        "        self.battery_max = battery_max\n",
        "        self.efficiency = efficiency\n",
        "        self.step_count = 0\n",
        "\n",
        "        # Charging nodes (binary array)\n",
        "        self.charging_nodes = np.zeros(self.n_nodes, dtype=np.int8)\n",
        "        for node in charging_nodes:\n",
        "            if 0 <= node < self.n_nodes:\n",
        "                self.charging_nodes[node] = 1\n",
        "\n",
        "        self.max_visited = self.n_nodes\n",
        "\n",
        "        # Observation space\n",
        "        self.observation_space = spaces.Dict({\n",
        "            \"current_node\": spaces.Discrete(self.n_nodes),\n",
        "            \"goal_node\": spaces.Discrete(self.n_nodes),\n",
        "            \"battery_level\": spaces.Box(low=0.0, high=self.battery_max, shape=(), dtype=np.float32),\n",
        "            \"neighbor_ids\": spaces.MultiDiscrete([self.n_nodes] * self.max_degree),\n",
        "            \"neighbor_distances\": spaces.Box(low=0.0, high=np.inf, shape=(self.max_degree,), dtype=np.float32),\n",
        "            \"neighbor_is_charging\": spaces.MultiBinary(self.max_degree),\n",
        "            \"visited_nodes\": spaces.MultiBinary(self.n_nodes),\n",
        "            \"neighbor_mask\": spaces.MultiBinary(self.max_degree)\n",
        "        })\n",
        "\n",
        "        self.action_space = spaces.Discrete(self.max_degree)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "        # For visualization\n",
        "        self.node_positions = None\n",
        "\n",
        "    def _get_reachable_neighbors(self, node):\n",
        "        neighbors = list(self.graph.neighbors(node))\n",
        "        neighbor_ids = [self.current_node] * self.max_degree\n",
        "        neighbor_dists = [0.0] * self.max_degree\n",
        "        neighbor_mask = [0] * self.max_degree\n",
        "        neighbor_is_charging = [0] * self.max_degree\n",
        "\n",
        "        # Debugging prints\n",
        "        #print(f\"Current node: {node}, Battery: {self.battery_level}\")\n",
        "        #print(f\"Neighbors: {neighbors}\")\n",
        "\n",
        "        valid_idx = 0\n",
        "        for neighbor in neighbors:\n",
        "            dist = distance_between(self.graph, self.current_node, neighbor)\n",
        "            consumption = dist * self.efficiency\n",
        "            #print(f\"  Neighbor: {neighbor}, Distance: {dist}, Consumption: {consumption}, Reachable: {self.battery_level - consumption >= 0}\")\n",
        "            if self.battery_level - dist * self.efficiency >= 0:\n",
        "                neighbor_ids[valid_idx] = neighbor\n",
        "                neighbor_dists[valid_idx] = dist\n",
        "                neighbor_mask[valid_idx] = 1\n",
        "                neighbor_is_charging[valid_idx] = self.charging_nodes[neighbor]\n",
        "                valid_idx += 1\n",
        "\n",
        "        #print(f\"Valid neighbors: {[neighbor_ids[i] for i in range(valid_idx)]}\")\n",
        "        return neighbor_ids, neighbor_dists, neighbor_mask, neighbor_is_charging\n",
        "\n",
        "    def _get_obs(self):\n",
        "        neighbor_ids, neighbor_dists, neighbor_mask, neighbor_is_charging = self._get_reachable_neighbors(self.current_node)\n",
        "        return {\n",
        "            \"current_node\": self.current_node,\n",
        "            \"goal_node\": self.goal_node,\n",
        "            \"battery_level\": np.array(self.battery_level, dtype=np.float32),\n",
        "            \"neighbor_ids\": np.array(neighbor_ids, dtype=np.int32),\n",
        "            \"neighbor_distances\": np.array(neighbor_dists, dtype=np.float32),\n",
        "            \"neighbor_is_charging\": np.array(neighbor_is_charging, dtype=np.int8),\n",
        "            \"visited_nodes\": self.visited_nodes,\n",
        "            \"neighbor_mask\": np.array(neighbor_mask, dtype=np.int8)\n",
        "        }\n",
        "\n",
        "    def _visit_current_node(self):\n",
        "        self.visited_nodes[self.current_node] = 1\n",
        "        self.step_count += 1\n",
        "\n",
        "    def reset(self, start_node=None, goal_node=None):\n",
        "        if start_node is not None:\n",
        "            self.start_node = start_node\n",
        "        if goal_node is not None:\n",
        "            self.goal_node = goal_node\n",
        "\n",
        "        # Ensure start node has valid actions\n",
        "        max_attempts = 10\n",
        "        for _ in range(max_attempts):\n",
        "            self.current_node = self.start_node if self.start_node is not None else np.random.randint(self.n_nodes)\n",
        "            self.goal_node = self.goal_node if self.goal_node is not None else np.random.randint(self.n_nodes)\n",
        "            while self.goal_node == self.current_node:\n",
        "                self.goal_node = np.random.randint(self.n_nodes)\n",
        "\n",
        "            self.step_count = 0\n",
        "            self.battery_level = self.battery_start\n",
        "            self.visited_nodes = np.zeros(self.n_nodes, dtype=np.int8)\n",
        "            self._visit_current_node()\n",
        "\n",
        "            # Check if there are valid actions\n",
        "            neighbor_ids, neighbor_dists, neighbor_mask, _ = self._get_reachable_neighbors(self.current_node)\n",
        "            valid_actions = [i for i, mask in enumerate(neighbor_mask) if mask == 1]\n",
        "            if valid_actions:\n",
        "                break\n",
        "        else:\n",
        "            print(\"Warning: Could not find a start node with valid actions after max attempts\")\n",
        "            self.current_node = self.start_node if self.start_node is not None else np.random.randint(self.n_nodes)\n",
        "            self.goal_node = self.goal_node if self.goal_node is not None else np.random.randint(self.n_nodes)\n",
        "            while self.goal_node == self.current_node:\n",
        "                self.goal_node = np.random.randint(self.n_nodes)\n",
        "            self.step_count = 0\n",
        "            self.battery_level = self.battery_start\n",
        "            self.visited_nodes = np.zeros(self.n_nodes, dtype=np.int8)\n",
        "            self._visit_current_node()\n",
        "\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action):\n",
        "        obs = self._get_obs()\n",
        "\n",
        "        neighbor_ids = obs[\"neighbor_ids\"]\n",
        "        neighbor_dists = obs[\"neighbor_distances\"]\n",
        "        neighbor_mask = obs[\"neighbor_mask\"]\n",
        "        neighbor_is_charging = obs[\"neighbor_is_charging\"]\n",
        "\n",
        "        done = False\n",
        "        info = {}\n",
        "        reward = 0\n",
        "\n",
        "        # Validate the action\n",
        "        if (\n",
        "            action < 0 or\n",
        "            action >= self.max_degree or\n",
        "            neighbor_mask[action] == 0 or\n",
        "            neighbor_ids[action] == self.current_node ):\n",
        "            reward = -1000\n",
        "            info[\"reason\"] = \"invalid_action\"\n",
        "            return obs, reward, done, info\n",
        "\n",
        "        next_node = neighbor_ids[action]\n",
        "        dist = neighbor_dists[action]\n",
        "\n",
        "        if ((self.battery_level - dist * self.efficiency < 0) or\n",
        "            ((self.charging_nodes[neighbor_ids[action]] == 0 or\n",
        "             neighbor_ids[action] != self.goal_node ) and\n",
        "            (self.battery_level ==  dist * self.efficiency))) :\n",
        "            reward = -10000\n",
        "            info[\"reason\"] = \"battery_insufficient_for_move\"\n",
        "            return obs, reward, done, info\n",
        "\n",
        "        # Battery cost\n",
        "        self.battery_level -= dist * self.efficiency\n",
        "\n",
        "        # Move to next node\n",
        "        self.current_node = next_node\n",
        "\n",
        "        # Base reward (movement cost)\n",
        "        reward = -60\n",
        "\n",
        "        # Penalize if revisiting a node\n",
        "        if self.visited_nodes[self.current_node] == 1:\n",
        "            reward -= 100\n",
        "            info[\"revisit_penalty\"] = True\n",
        "\n",
        "        # Recharge if on a charging node\n",
        "        if self.charging_nodes[self.current_node]:\n",
        "            self.battery_level = self.battery_max\n",
        "            reward += 10\n",
        "\n",
        "        self._visit_current_node()\n",
        "\n",
        "        # Check terminal conditions\n",
        "        if self.current_node == self.goal_node:\n",
        "            reward += 1000\n",
        "            done = True\n",
        "            info[\"reason\"] = \"goal_reached\"\n",
        "\n",
        "        elif self.step_count >= self.max_steps:\n",
        "            reward -= 60\n",
        "            done = True\n",
        "            info[\"reason\"] = \"max_steps_reached\"\n",
        "\n",
        "        elif self.battery_level <= 0:\n",
        "            reward -= 1000\n",
        "            done = False\n",
        "            info[\"reason\"] = \"battery_depleted\"\n",
        "\n",
        "        new_obs = self._get_obs()\n",
        "        if self.step_count == self.max_steps:\n",
        "            done = True\n",
        "        return new_obs, reward, done, info\n",
        "    def render(self, mode: str = 'human') -> None:\n",
        "        if self.node_positions is None:\n",
        "            try:\n",
        "                self.node_positions = {node: (data['x'], data['y'])\n",
        "                                     for node, data in self.graph.nodes(data=True)}\n",
        "            except KeyError:\n",
        "                self.node_positions = nx.spring_layout(self.graph)\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        nx.draw_networkx_edges(self.graph, self.node_positions, alpha=0.3, edge_color='gray')\n",
        "        charging_nodes_list = [node for node in range(len(self.charging_nodes)) if self.charging_nodes[node] == 1]\n",
        "        nx.draw_networkx_nodes(self.graph, self.node_positions, nodelist=charging_nodes_list,\n",
        "                              node_color='green', node_size=100, label='Charging Station')\n",
        "        current_node = self.current_node\n",
        "        nx.draw_networkx_nodes(self.graph, self.node_positions, nodelist=[current_node],\n",
        "                              node_color='blue', node_size=200, label='Current Position')\n",
        "        goal_node = self.goal_node\n",
        "        nx.draw_networkx_nodes(self.graph, self.node_positions, nodelist=[goal_node],\n",
        "                              node_color='red', node_size=200, label='Goal')\n",
        "        visited_nodes = [node for node in range(len(self.visited_nodes))\n",
        "                        if self.visited_nodes[node] == 1 and node != current_node]\n",
        "        nx.draw_networkx_nodes(self.graph, self.node_positions, nodelist=visited_nodes,\n",
        "                              node_color='lightblue', node_size=50, label='Visited')\n",
        "        plt.title(f\"Step: {self.step_count}, Battery: {self.battery_level:.1f}\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHTyoCyra7dv"
      },
      "source": [
        "# DQANAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fGYkCvDx27HD"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, graph_size, max_battery):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.graph_size = graph_size\n",
        "        self.max_battery = max_battery\n",
        "        self.gamma = 0.9\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.1\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.01\n",
        "        self.update_target_freq = 100\n",
        "        self.batch_size = 32\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "        self.train_step_counter = 0\n",
        "\n",
        "    def _build_model(self):\n",
        "        current_node_input = keras.Input(shape=(1,), name='current_node')\n",
        "        goal_node_input = keras.Input(shape=(1,), name='goal_node')\n",
        "        battery_input = keras.Input(shape=(1,), name='battery_level')\n",
        "        visited_nodes_input = keras.Input(shape=(self.graph_size,), name='visited_nodes')\n",
        "        embedding_dim = min(64, self.graph_size // 4)\n",
        "        current_embedding = layers.Embedding(self.graph_size, embedding_dim)(current_node_input)\n",
        "        goal_embedding = layers.Embedding(self.graph_size, embedding_dim)(goal_node_input)\n",
        "        current_flat = layers.Flatten()(current_embedding)\n",
        "        goal_flat = layers.Flatten()(goal_embedding)\n",
        "        battery_normalized = layers.Lambda(lambda x: x / self.max_battery)(battery_input)\n",
        "        visited_expanded = layers.Reshape((self.graph_size, 1))(visited_nodes_input)\n",
        "        visited_features = layers.Conv1D(16, 3, padding='same', activation='relu')(visited_expanded)\n",
        "        visited_features = layers.MaxPooling1D(2)(visited_features)\n",
        "        visited_features = layers.Conv1D(32, 3, padding='same', activation='relu')(visited_features)\n",
        "        visited_features = layers.GlobalAveragePooling1D()(visited_features)\n",
        "        combined = layers.Concatenate()([current_flat, goal_flat, battery_normalized, visited_features])\n",
        "        x = layers.Dense(256, activation='relu')(combined)\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "        x = layers.Dense(64, activation='relu')(x)\n",
        "        output = layers.Dense(self.action_size, activation='linear')(x)\n",
        "        model = keras.Model(inputs=[current_node_input, goal_node_input, battery_input, visited_nodes_input], outputs=output)\n",
        "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def memorize(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state, valid_actions):\n",
        "        if len(valid_actions) == 0:\n",
        "            return -1\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(valid_actions)\n",
        "        q_values = self.predict(state)\n",
        "        mask = np.ones(self.action_size) * -np.inf\n",
        "        mask[valid_actions] = 0\n",
        "        q_values = q_values + mask\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "    def predict(self, state):\n",
        "        current_node = np.array([[state['current_node']]])\n",
        "        goal_node = np.array([[state['goal_node']]])\n",
        "        battery_level = np.array([[state['battery_level']]])\n",
        "        visited_nodes = np.array([state['visited_nodes']])\n",
        "        return self.model.predict([current_node, goal_node, battery_level, visited_nodes], verbose=0)[0]\n",
        "\n",
        "    def replay(self, batch_size=None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.batch_size\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        current_states_current_node = []\n",
        "        current_states_goal_node = []\n",
        "        current_states_battery = []\n",
        "        current_states_visited = []\n",
        "        next_states_current_node = []\n",
        "        next_states_goal_node = []\n",
        "        next_states_battery = []\n",
        "        next_states_visited = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        dones = []\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            current_states_current_node.append(state['current_node'])\n",
        "            current_states_goal_node.append(state['goal_node'])\n",
        "            current_states_battery.append(state['battery_level'])\n",
        "            current_states_visited.append(state['visited_nodes'])\n",
        "            next_states_current_node.append(next_state['current_node'])\n",
        "            next_states_goal_node.append(next_state['goal_node'])\n",
        "            next_states_battery.append(next_state['battery_level'])\n",
        "            next_states_visited.append(next_state['visited_nodes'])\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            dones.append(done)\n",
        "        current_states_current_node = np.array(current_states_current_node)\n",
        "        current_states_goal_node = np.array(current_states_goal_node)\n",
        "        current_states_battery = np.array(current_states_battery)\n",
        "        current_states_visited = np.array(current_states_visited)\n",
        "        next_states_current_node = np.array(next_states_current_node)\n",
        "        next_states_goal_node = np.array(next_states_goal_node)\n",
        "        next_states_battery = np.array(next_states_battery)\n",
        "        next_states_visited = np.array(next_states_visited)\n",
        "        actions = np.array(actions)\n",
        "        rewards = np.array(rewards)\n",
        "        dones = np.array(dones)\n",
        "        current_q_values = self.model.predict([\n",
        "            current_states_current_node.reshape(-1, 1),\n",
        "            current_states_goal_node.reshape(-1, 1),\n",
        "            current_states_battery.reshape(-1, 1),\n",
        "            current_states_visited\n",
        "        ], verbose=0)\n",
        "        next_q_values = self.target_model.predict([\n",
        "            next_states_current_node.reshape(-1, 1),\n",
        "            next_states_goal_node.reshape(-1, 1),\n",
        "            next_states_battery.reshape(-1, 1),\n",
        "            next_states_visited\n",
        "        ], verbose=0)\n",
        "        for i in range(batch_size):\n",
        "            if dones[i]:\n",
        "                current_q_values[i, actions[i]] = rewards[i]\n",
        "            else:\n",
        "                current_q_values[i, actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n",
        "        self.model.fit([\n",
        "            current_states_current_node.reshape(-1, 1),\n",
        "            current_states_goal_node.reshape(-1, 1),\n",
        "            current_states_battery.reshape(-1, 1),\n",
        "            current_states_visited\n",
        "        ], current_q_values, epochs=1, verbose=0)\n",
        "        self.train_step_counter += 1\n",
        "        if self.train_step_counter % self.update_target_freq == 0:\n",
        "            self.update_target_model()\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTXBOmD_a4e8"
      },
      "source": [
        "# Trainning Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c1xmZwSE_jfO"
      },
      "outputs": [],
      "source": [
        "def create_test_graph():\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Step 1: Add 100 nodes\n",
        "    for i in range(100):\n",
        "        G.add_node(i)\n",
        "\n",
        "    # Step 2: Add random edges (each node connects to a few others)\n",
        "    for i in range(100):\n",
        "        connections = random.sample(range(100), k=random.randint(2, 5))  # Connect to 2–5 random nodes\n",
        "        for j in connections:\n",
        "            if i != j and not G.has_edge(i, j):  # Avoid loops and duplicates\n",
        "                length = round(random.uniform(5.0, 20.0), 1)  # Random length between 5 and 20\n",
        "                G.add_edge(i, j, length=length)\n",
        "\n",
        "    # Debug: Print sample edges\n",
        "    print(\"Sample of graph edges and attributes:\")\n",
        "    for i, (u, v, data) in enumerate(G.edges(data=True)):\n",
        "        print(f\"Edge ({u}, {v}): {data}\")\n",
        "        if i > 20:  # Just print a few for readability\n",
        "            break\n",
        "\n",
        "    # Step 3: Choose 10 random charging stations\n",
        "    charging_nodes = random.sample(range(100), 10)\n",
        "\n",
        "    return G, charging_nodes\n",
        "def run_dqn_episodes(num_episodes=100, render_every=2):\n",
        "    G, charging_nodes = create_test_graph()\n",
        "    env = GraphEnv(\n",
        "        graph=G,\n",
        "        charging_nodes=charging_nodes,\n",
        "        battery_max=100.0,\n",
        "        battery_start=50.0,\n",
        "        efficiency=0.1,\n",
        "        max_steps=100\n",
        "    )\n",
        "    print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
        "    print(f\"Number of charging stations: {len(charging_nodes)}\")\n",
        "    print(f\"Max degree: {env.max_degree}\")\n",
        "    state_size = 2 + 1 + G.number_of_nodes()\n",
        "    action_size = env.max_degree\n",
        "    agent = DQNAgent(\n",
        "        state_size=state_size,\n",
        "        action_size=action_size,\n",
        "        graph_size=G.number_of_nodes(),\n",
        "        max_battery=env.battery_max\n",
        "    )\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    goal_reached = []\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nEpisode {episode+1}/{num_episodes}\")\n",
        "        state = env.reset()\n",
        "        print(f\"Start node: {state['current_node']}, Goal node: {state['goal_node']}\")\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        done = False\n",
        "        info = {}\n",
        "        while not done:\n",
        "            neighbor_ids = state['neighbor_ids']\n",
        "            neighbor_mask = state['neighbor_mask']\n",
        "            valid_actions = [i for i, mask in enumerate(neighbor_mask) if mask == 1 and neighbor_ids[i] != state['current_node']]\n",
        "            if not valid_actions:\n",
        "                print(\"No valid actions available. Terminating episode.\")\n",
        "                info = {\"reason\": \"no_valid_actions\"}\n",
        "                break\n",
        "            action = agent.act(state, valid_actions)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            agent.memorize(state, action, reward, next_state, done)\n",
        "            agent.replay()\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "            if steps % 5 == 0:\n",
        "                print(f\"Step {steps}: Current node: {state['current_node']}, Battery: {state['battery_level']:.1f}, Reward: {total_reward}\")\n",
        "            if episode % render_every == 0 and steps % 10 == 0:\n",
        "                env.render()\n",
        "            if done or steps >= env.max_steps:\n",
        "                break\n",
        "        episode_rewards.append(total_reward)\n",
        "        episode_lengths.append(steps)\n",
        "        goal_reached.append(True if info.get('reason') == 'goal_reached' else False)\n",
        "        print(f\"Episode {episode+1} completed:\")\n",
        "        print(f\"  Steps: {steps}\")\n",
        "        print(f\"  Total reward: {total_reward}\")\n",
        "        print(f\"  Reached goal: {'Yes' if goal_reached[-1] else 'No'}\")\n",
        "        if 'reason' in info:\n",
        "            print(f\"  Reason: {info['reason']}\")\n",
        "    print(\"\\nTraining Summary:\")\n",
        "    print(f\"Episodes run: {num_episodes}\")\n",
        "    print(f\"Goal reached: {sum(goal_reached)}/{num_episodes} episodes\")\n",
        "    print(f\"Average reward: {np.mean(episode_rewards):.2f}\")\n",
        "    print(f\"Average episode length: {np.mean(episode_lengths):.2f} steps\")\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(episode_rewards)\n",
        "    plt.title('Episode Rewards')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(episode_lengths)\n",
        "    plt.title('Episode Lengths')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Steps')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return agent, env\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5i_eX1iyX9Sd",
        "outputId": "12ff4a97-cfbb-484e-90df-995d248a091d"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    run_dqn_episodes(num_episodes=50, render_every=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mph_sQrNX9on"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
